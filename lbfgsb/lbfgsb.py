"""
L-BFGS-B is a code for solving large nonlinear optimization problems with 
simple bounds on the variables.

The code can also be used for unconstrained problems and is as efficient for 
these problems as the earlier limited memory code L-BFGS.

References:

  [1] R. H. Byrd, P. Lu, J. Nocedal and C. Zhu, ``A limited
  memory algorithm for bound constrained optimization'',
  SIAM J. Scientific Computing 16 (1995), no. 5, pp. 1190--1208.

  [2] C. Zhu, R.H. Byrd, P. Lu, J. Nocedal, ``L-BFGS-B: FORTRAN
  Subroutines for Large Scale Bound Constrained Optimization''
  Tech. Report, NAM-11, EECS Department, Northwestern University,
  1994.

  [3] R. Byrd, J. Nocedal and R. Schnabel "Representations of
  Quasi-Newton Matrices and their use in Limited Memory Methods'',
  Mathematical Programming 63 (1994), no. 4, pp. 129-156.

  (Postscript files of these papers are available via anonymous
   ftp to eecs.nwu.edu in the directory pub/lbfgs/lbfgs_bcm.)

Python Interface:

Author: Aaron Parsons
Date: 10/21/06
Revisions: None
"""

__lbfgsb_version__ = '2.1'
__version__ = '0.0.3'


import _lbfgsb, numpy

# __     __         
# \ \   / /_ _ _ __ 
#  \ \ / / _` | '__|
#   \ V / (_| | |   
#    \_/ \__,_|_|   
                  
class Var:
    """A class containing information about the start value and bounds of
    a variable which is being fit by LBFGSB."""
    def __init__(self, start_val, lower_limit=None, upper_limit=None):
        self.x = start_val
        self.nbd = 0
        self.l = 0
        self.u = 0
        if lower_limit is not None:
            self.l = lower_limit
            self.nbd = self.nbd | 1
        if upper_limit is not None:
            self.u = upper_limit
            self.nbd = self.nbd | 2

#            _       _           _         
#  _ __ ___ (_)_ __ (_)_ __ ___ (_)_______ 
# | '_ ` _ \| | '_ \| | '_ ` _ \| |_  / _ \
# | | | | | | | | | | | | | | | | |/ /  __/
# |_| |_| |_|_|_| |_|_|_| |_| |_|_/___\___|

def minimize(fun, vars, gradfun=None, m=5, factr=1e7, pgtol=1e-5, iprint=-1):
    """Minimize the python function 'fun'.
        
        'fun' is a function which accepts as many variables as are listed
        in 'vars'.

        'gradfun' is a function of the same number of variables which returns 
        the (multidimensional) gradient of 'fun' at a location.  If None is 
        provided, it is assumed that fun which return both the evaluation of 
        the function and the gradient (this is useful for cases where you only
        want to calculate data once, and use it to simultaneously compute
        the function and the gradient).
        
        'vars' is a list of Var objects which specify various fitting 
        constraints for each fit variable.

        'm' is an integer number of corrections used in the limited memory 
        matrix.  Values of m < 3  are not recommended, and large values of m 
        can result in excessive computing time. The range  3 <= m <= 20 is 
        recommended.

        'factr' is a float representing a tolerance in the termination test 
        for the algorithm.  The iteration will stop when
           (f^k - f^{k+1})/max{|f^k|,|f^{k+1}|,1} <= factr*epsmch
        where epsmch is the machine precision which is automatically
        generated by the code. Typical values for factr on a computer
        with 15 digits of accuracy in double precision are:
        factr=1.d+12 for low accuracy;
              1.d+7  for moderate accuracy;
              1.d+1  for extremely high accuracy.
        The user can suppress this termination test by setting factr=0.

        'pgtol' is a float >= 0.  The iteration will stop when
                  max{|proj g_i | i = 1, ..., n} <= pgtol
          where pg_i is the ith component of the projected gradient.
        The user can suppress this termination test by setting pgtol=0.

        'iprint' is an integer which controls the frequency and type of 
        output generated:
         iprint<0    no output is generated;
         iprint=0    print only one line at the last iteration;
         0<iprint<99 print also f and |proj g| every iprint iterations;
         iprint=99   print details of every iteration except n-vectors;
         iprint=100  print also the changes of active set and final x;
         iprint>100  print details of every iteration including x and g;
        When iprint > 0, the file iterate.dat will be created to
                         summarize the iteration.
    """
    n = len(vars)
    x = numpy.array([v.x for v in vars], dtype=numpy.double)
    l = numpy.array([v.l for v in vars], dtype=numpy.double)
    u = numpy.array([v.u for v in vars], dtype=numpy.double)
    nbd = numpy.array([v.nbd for v in vars], dtype=numpy.int)
    lsave = numpy.zeros((4,), dtype=numpy.bool)
    isave = numpy.zeros((44,), dtype=numpy.int)
    dsave = numpy.zeros((29,), dtype=numpy.double)
    task = 'START'
    csave = numpy.array('CSAVE')
    wa = numpy.zeros((2*m*n+4*n+12*m**2+12*m,), dtype=numpy.double)
    iwa = numpy.zeros((3*n,), dtype=numpy.int)
    f, g = 0., numpy.zeros_like(x)
    while True:
        x, task = _lbfgsb.setulb(m, x, l, u, nbd, f, g, pgtol, wa, iwa, \
            task, csave, lsave, isave, dsave, iprint=iprint, factr=factr)
        if task[:2] == 'FG':
            if gradfun is None:
                f, g = apply(fun, x)
                print x, f
                g = numpy.array(g, dtype=numpy.double)
            else:
                f = apply(fun, x)
                g = numpy.array(apply(gradfun, x), dtype=numpy.double)
        elif task[:5] == 'NEW_X': pass
        else: break
    return x, f

#  _                _                                                __ _ _   
# | | ___  __ _ ___| |_     ___  __ _ _   _  __ _ _ __ ___  ___     / _(_) |_ 
# | |/ _ \/ _` / __| __|   / __|/ _` | | | |/ _` | '__/ _ \/ __|   | |_| | __|
# | |  __/ (_| \__ \ |_    \__ \ (_| | |_| | (_| | | |  __/\__ \   |  _| | |_ 
# |_|\___|\__,_|___/\__|___|___/\__, |\__,_|\__,_|_|  \___||___/___|_| |_|\__|
#                     |_____|      |_|                        |_____|         

def lsq_fit(f, vars, xs, ys, g=None, m=5, factr=1e7, 
    pgtol=1e-5, iprint=-1):
    """Perform a least-squares fit of a function to measured data.
    f:      The function to be fit, and should accept the variables to be
            fit (vars) and the datapoint locations (xs), in that order.
    vars:   The parameters to be fit (see docs for minimize).
    xs:     The locations at which data has been measured.
    ys:     The measured values of the function f at the locations listed 
            in xs.
    g:      The gradient of f, and should accept the same arguments as f.  It
            should return gradient of f in each parameter, in the order listed 
            in vars.  If g is not provided, one will be calculated very 
            inefficiently by perturbing each variable in f and summing over 
            all datapoints.
    The other parameters are described in the documentation for 'minimize'."""
    ays = numpy.array(ys)
    if g is not None:
        def fit_func(*args):
            est_ys = numpy.array([apply(f, args + x) for x in xs])
            est_dys = numpy.array([apply(g, args + x) for x in xs])
            est_dys = est_dys.transpose()
            return sum(abs(est_ys - ays)**2), \
                2*numpy.sum(est_dys * (est_ys - ays), axis=1)
    else:
        def fit_func(*args):
            est_ys, est_dys = [], []
            for x in xs:
                ey, edy = apply(f, args + (x,))
                est_ys.append(ey)
                est_dys.append(edy)
            est_ys = numpy.array(est_ys)
            est_dys = numpy.array(est_dys)
            est_dys = est_dys.transpose()
            return sum(abs(est_ys - ays)**2), \
                2*numpy.sum(est_dys * (est_ys - ays), axis=1)
    fits, chisq = minimize(fit_func, vars, m=m, factr=factr, pgtol=pgtol, 
        iprint=iprint)
    return fits, chisq

def gen_grad(f, pgtol):
    """Inefficiently take the derivative of f."""
    def grad(*args):
        args = numpy.array(args)
        L = []
        for i in range(len(args)):
            nargs = args.copy()
            nargs[i] += pgtol
            L.append((apply(f,nargs) - apply(f,args)) / pgtol)
        return numpy.array(L)
    return grad

def poly_fit(in_poly, xs, ys, m=5, factr=1e7, pgtol=1e-5, iprint=-1):
    """Fit a polynomial to xs, ys, using in_poly as a starting point."""
    order = len(in_poly)
    print order
    vars = []
    for i in in_poly: vars.append(Var(i))
    pwrs = numpy.arange(order)
    def poly_func(*args):
        poly = numpy.array(args[:order])
        grad = args[-1]**pwrs
        return sum(poly*grad), grad
    return lsq_fit(poly_func, vars, xs, ys, m=m, factr=factr, pgtol=pgtol,
        iprint=iprint)
        
#  _            _   _                     _     
# | |_ ___  ___| |_| |__   ___ _ __   ___| |__  
# | __/ _ \/ __| __| '_ \ / _ \ '_ \ / __| '_ \ 
# | ||  __/\__ \ |_| |_) |  __/ | | | (__| | | |
#  \__\___||___/\__|_.__/ \___|_| |_|\___|_| |_|
                                              
if __name__ == '__main__':
    import math
    def f(a, b, c, x): return a*x**b + c
    def g(a, b, c, x): return (x**b, a*math.log(x)*x**b, 1)
    abc = (5., -2., 3.)
    xs = ((1,), (2,), (3,), (4,))
    ys = [apply(f, abc + x) for x in xs]
    vars = [Var(1, 0), Var(1), Var(1)]
    fits, chisq = lsq_fit(f, vars, xs, ys, g=g)
    print fits
    print chisq

